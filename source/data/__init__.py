###############################################################################
# This file contains basic functions: find_dataset_using_name, create_dataset, 
# train_val_split, ... which is used to create the dataset for training
###############################################################################

import importlib
import torch.utils.data
from data.base_dataset import BaseDataset

def find_dataset_using_name(dataset_name):
    dataset_filename = "data." + "dataset_" + dataset_name
    datasetlib = importlib.import_module(dataset_filename)
    dataset = None
    target_dataset_name = dataset_name.replace('_', '') + 'dataset'
    for name, cls in datasetlib.__dict__.items():
        if name.lower() == target_dataset_name.lower() and issubclass(cls, BaseDataset):
            dataset = cls
           
    if dataset is None:
        print("In %s.py, there should be a subclass of BaseDataset with class name that matches %s in lowercase." % (dataset_filename, target_dataset_name))
        exit(0)
    return dataset

def create_dataset(opt):
    dataset = find_dataset_using_name(opt.dataset_mode)
    instance = dataset()
    instance.initialize(opt)
    print("dataset [%s] was created" % (instance.name()))
    return instance

class CustomDatasetDataLoader():
    """This class organizes dataset which was generated by 
    create_dataset() function. 
    Note: self.working_subset will be called in train.py to
    evoke the kind of dataset (main, valid) to be used  
    """
        
    def name(self):
        return 'CustomDatasetDataLoader'

    def __init__(self, opt):
        self.opt = opt
        self.working_subset = 'main'
        self.mainDataset = create_dataset(opt)
        self.mainDataloader = torch.utils.data.DataLoader(
            self.mainDataset,
            batch_size=opt.batch_size,
            shuffle=not opt.serial_batches,
            num_workers=int(opt.num_threads)) 
        
    def load_data(self):
        return self

    def __len__(self):
        return min(len(getattr(self, 'mainDataset')), self.opt.max_dataset_size)

    def __iter__(self):
        for i, data in enumerate(getattr(self, 'mainDataloader')):
            if i * self.opt.batch_size >= self.opt.max_dataset_size:
                break
            yield data